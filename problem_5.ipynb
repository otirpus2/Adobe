{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Problem 5</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Object Detection with YOLO - Documentation</h1>\n",
    "    <p>Below is the documentation for the Python script using the Ultralytics YOLO library to perform object detection on images in a specified folder:</p>\n",
    "    \n",
    "<h2>Code Overview</h2>\n",
    "<ol>\n",
    "    <li>Import necessary libraries including <code>os</code>, <code>shutil</code>, <code>itertools</code>, <code>matplotlib.pyplot</code>, <code>YOLO</code> from Ultralytics, and <code>clip</code> from OpenAI.</li>\n",
    "    <li>Define the function <code>detect_and_save_objects(yolo_model, input_folder, output_folder)</code> to perform object detection on images and save detected object crops.</li>\n",
    "    <li>Define the function <code>compare_images_using_clip(clip_model, output_folder)</code> to compare images using the CLIP model and save similar object crops.</li>\n",
    "    <li>Define the main function <code>main()</code> to orchestrate the execution of object detection and image comparison.</li>\n",
    "    <li>Call the <code>main()</code> function when the script is run as the main module.</li>\n",
    "</ol>\n",
    "\n",
    "<h2>Execution Flow</h2>\n",
    "<ol>\n",
    "    <li>Create a YOLO model instance using the pretrained model file <code>yolov8m.pt</code>.</li>\n",
    "    <li>Specify input and output directories for images and results.</li>\n",
    "    <li>Call the <code>detect_and_save_objects</code> function to perform object detection and save object crops.</li>\n",
    "    <li>Call the <code>compare_images_using_clip</code> function to compare images and save similar object crops.</li>\n",
    "</ol>\n",
    "\n",
    "<h2>Main Execution</h2>\n",
    "<ul>\n",
    "    <li>Check if the script is being run as the main module using <code>if __name__ == \"__main__\":</code></li>\n",
    "    <li>Call the <code>main()</code> function to initiate the execution of object detection and comparison.</li>\n",
    "    <li>Handle exceptions and print error messages in case of failures.</li>\n",
    "</ul>\n",
    "    \n",
    "<p>This documentation provides an overview of the code's functionality and execution process.</p>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install ultralytics\n",
    "!pip install torch\n",
    "!pip install clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 d:\\Adobe\\Aithon\\Aithon\\All_Images\\1.jpg: 384x640 4 persons, 1 couch, 285.4ms\n",
      "Speed: 7.5ms preprocess, 285.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 d:\\Adobe\\Aithon\\Aithon\\All_Images\\2.jpg: 448x640 3 persons, 1 bed, 194.2ms\n",
      "Speed: 8.2ms preprocess, 194.2ms inference, 3.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 d:\\Adobe\\Aithon\\Aithon\\All_Images\\3.jpg: 448x640 3 persons, 213.3ms\n",
      "Speed: 7.5ms preprocess, 213.3ms inference, 5.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 d:\\Adobe\\Aithon\\Aithon\\All_Images\\AdobeStock_112814949.jpeg: 448x640 1 spoon, 1 bowl, 1 sandwich, 320.2ms\n",
      "Speed: 7.1ms preprocess, 320.2ms inference, 4.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 d:\\Adobe\\Aithon\\Aithon\\All_Images\\AdobeStock_119085612.jpeg: 480x640 1 cup, 1 toothbrush, 289.4ms\n",
      "Speed: 6.0ms preprocess, 289.4ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 d:\\Adobe\\Aithon\\Aithon\\All_Images\\AdobeStock_189740072.jpeg: 448x640 1 person, 312.8ms\n",
      "Speed: 26.5ms preprocess, 312.8ms inference, 3.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 d:\\Adobe\\Aithon\\Aithon\\All_Images\\AdobeStock_254640691.jpeg: 448x640 1 spoon, 4 bowls, 3 oranges, 1 dining table, 195.0ms\n",
      "Speed: 4.1ms preprocess, 195.0ms inference, 3.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 d:\\Adobe\\Aithon\\Aithon\\All_Images\\AdobeStock_255497590.jpeg: 352x640 1 bowl, 222.4ms\n",
      "Speed: 6.1ms preprocess, 222.4ms inference, 2.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "image 1/1 d:\\Adobe\\Aithon\\Aithon\\All_Images\\AdobeStock_257024669.jpeg: 448x640 1 suitcase, 265.5ms\n",
      "Speed: 6.2ms preprocess, 265.5ms inference, 4.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 d:\\Adobe\\Aithon\\Aithon\\All_Images\\AdobeStock_286178925.jpeg: 448x640 5 bowls, 6 donuts, 1 cake, 1 dining table, 200.9ms\n",
      "Speed: 8.1ms preprocess, 200.9ms inference, 3.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 d:\\Adobe\\Aithon\\Aithon\\All_Images\\AdobeStock_297274410.jpeg: 448x640 2 persons, 3 bowls, 1 dining table, 219.8ms\n",
      "Speed: 3.5ms preprocess, 219.8ms inference, 5.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 d:\\Adobe\\Aithon\\Aithon\\All_Images\\AdobeStock_315333860.jpeg: 448x640 3 persons, 1 wine glass, 4 cups, 1 spoon, 1 bowl, 4 pizzas, 1 dining table, 266.5ms\n",
      "Speed: 10.7ms preprocess, 266.5ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 d:\\Adobe\\Aithon\\Aithon\\All_Images\\AdobeStock_321810820.jpeg: 384x640 5 persons, 1 skateboard, 198.7ms\n",
      "Speed: 4.7ms preprocess, 198.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 d:\\Adobe\\Aithon\\Aithon\\All_Images\\AdobeStock_323628715.jpeg: 448x640 7 persons, 2 cups, 1 bowl, 4 pizzas, 1 dining table, 200.1ms\n",
      "Speed: 5.5ms preprocess, 200.1ms inference, 29.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 d:\\Adobe\\Aithon\\Aithon\\All_Images\\AdobeStock_349162891.jpeg: 608x640 1 sandwich, 361.5ms\n",
      "Speed: 9.2ms preprocess, 361.5ms inference, 4.5ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "image 1/1 d:\\Adobe\\Aithon\\Aithon\\All_Images\\AdobeStock_425653713.jpeg: 448x640 1 handbag, 1 bowl, 1 potted plant, 1 bed, 1 laptop, 1 keyboard, 259.2ms\n",
      "Speed: 12.5ms preprocess, 259.2ms inference, 5.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 d:\\Adobe\\Aithon\\Aithon\\All_Images\\AdobeStock_57849082.jpeg: 448x640 1 person, 2 pizzas, 1 dining table, 236.7ms\n",
      "Speed: 7.5ms preprocess, 236.7ms inference, 3.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 d:\\Adobe\\Aithon\\Aithon\\All_Images\\AdobeStock_58143201.jpeg: 480x640 (no detections), 255.6ms\n",
      "Speed: 5.5ms preprocess, 255.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 d:\\Adobe\\Aithon\\Aithon\\All_Images\\AdobeStock_584637362.jpeg: 448x640 2 airplanes, 3 suitcases, 255.1ms\n",
      "Speed: 8.5ms preprocess, 255.1ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 d:\\Adobe\\Aithon\\Aithon\\All_Images\\AdobeStock_594853635.jpeg: 224x640 8 sandwichs, 135.4ms\n",
      "Speed: 4.0ms preprocess, 135.4ms inference, 4.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "\n",
      "image 1/1 d:\\Adobe\\Aithon\\Aithon\\All_Images\\AdobeStock_608957878.jpeg: 448x640 2 persons, 2 toothbrushs, 203.6ms\n",
      "Speed: 5.2ms preprocess, 203.6ms inference, 2.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 d:\\Adobe\\Aithon\\Aithon\\All_Images\\AdobeStock_618914535.jpeg: 480x640 2 sandwichs, 218.1ms\n",
      "Speed: 5.6ms preprocess, 218.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 d:\\Adobe\\Aithon\\Aithon\\All_Images\\AdobeStock_92669497.jpeg: 448x640 7 persons, 1 suitcase, 208.4ms\n",
      "Speed: 3.0ms preprocess, 208.4ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 d:\\Adobe\\Aithon\\Aithon\\All_Images\\AdobeStock_96284524.jpeg: 448x640 12 donuts, 185.2ms\n",
      "Speed: 3.0ms preprocess, 185.2ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 d:\\Adobe\\Aithon\\Aithon\\All_Images\\DL.jpeg: 384x640 1 bottle, 193.4ms\n",
      "Speed: 3.6ms preprocess, 193.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 d:\\Adobe\\Aithon\\Aithon\\All_Images\\maxresdefault.jpeg: 384x640 3 persons, 196.8ms\n",
      "Speed: 6.0ms preprocess, 196.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 d:\\Adobe\\Aithon\\Aithon\\All_Images\\Mist.jpeg: 448x640 1 tie, 207.7ms\n",
      "Speed: 4.5ms preprocess, 207.7ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unexpected type <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 78\u001b[0m\n\u001b[0;32m     74\u001b[0m                 temp_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     77\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 78\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[14], line 14\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m model \u001b[39m=\u001b[39m YOLO(\u001b[39m\"\u001b[39m\u001b[39myolov8n.pt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m detect_objects_yolo(model, input_folder, output_folder)\n\u001b[1;32m---> 14\u001b[0m compare_images_clip(output_folder)\n",
      "Cell \u001b[1;32mIn[14], line 51\u001b[0m, in \u001b[0;36mcompare_images_clip\u001b[1;34m(output_folder)\u001b[0m\n\u001b[0;32m     49\u001b[0m complete_img_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(root, files[\u001b[39m0\u001b[39m])\n\u001b[0;32m     50\u001b[0m entity \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mfilter\u001b[39m(\u001b[39mlambda\u001b[39;00m z: \u001b[39mnot\u001b[39;00m z\u001b[39m.\u001b[39misdigit(), files[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]))\n\u001b[1;32m---> 51\u001b[0m input_image \u001b[39m=\u001b[39m clip_preprocess(plt\u001b[39m.\u001b[39;49mimread(complete_img_path))[\u001b[39mNone\u001b[39;00m]\n\u001b[0;32m     52\u001b[0m input_image_features \u001b[39m=\u001b[39m clip_model\u001b[39m.\u001b[39mencode_image(input_image)\n\u001b[0;32m     53\u001b[0m results \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\transforms\\transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m    354\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[39m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mresize(img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mantialias)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\transforms\\functional.py:476\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[39mif\u001b[39;00m max_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(size) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    471\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    472\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mmax_size should only be passed if size specifies the length of the smaller edge, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    473\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    474\u001b[0m         )\n\u001b[1;32m--> 476\u001b[0m _, image_height, image_width \u001b[39m=\u001b[39m get_dimensions(img)\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(size, \u001b[39mint\u001b[39m):\n\u001b[0;32m    478\u001b[0m     size \u001b[39m=\u001b[39m [size]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\transforms\\functional.py:78\u001b[0m, in \u001b[0;36mget_dimensions\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mget_dimensions(img)\n\u001b[1;32m---> 78\u001b[0m \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mget_dimensions(img)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\transforms\\_functional_pil.py:31\u001b[0m, in \u001b[0;36mget_dimensions\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     29\u001b[0m     width, height \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39msize\n\u001b[0;32m     30\u001b[0m     \u001b[39mreturn\u001b[39;00m [channels, height, width]\n\u001b[1;32m---> 31\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(img)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Unexpected type <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import clip\n",
    "\n",
    "\n",
    "def main():\n",
    "    input_folder = \"./All_Images\"\n",
    "    output_folder = \"output/problem5\"\n",
    "    model = YOLO(\"yolov8n.pt\")\n",
    "    detect_objects_yolo(model, input_folder, output_folder)\n",
    "    compare_images_clip(output_folder)\n",
    "\n",
    "\n",
    "def detect_objects_yolo(model ,input_folder, output_folder):\n",
    "    \n",
    "    image_paths = [os.path.join(input_folder, img) for img in os.listdir(input_folder) if img.lower().endswith(('.jpeg', '.jpg'))]\n",
    "\n",
    "    for img_path in image_paths:\n",
    "        image_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        image_output_folder = os.path.join(output_folder, image_name)\n",
    "        os.makedirs(image_output_folder, exist_ok=True)\n",
    "\n",
    "        results = model.predict(img_path, save=False)\n",
    "        temp_store_label_no = {}\n",
    "\n",
    "        for box in results[0].boxes:\n",
    "            label_idx = box.cls[0].item()\n",
    "            label = results[0].names[label_idx]\n",
    "            entity_folder = os.path.join(image_output_folder, label)\n",
    "            temp_store_label_no[label] = temp_store_label_no.get(label, 0) + 1\n",
    "            os.makedirs(entity_folder, exist_ok=True)\n",
    "\n",
    "            x_min, y_min, x_max, y_max = box.xyxy[0]\n",
    "            x_min, y_min, x_max, y_max = x_min.item(), y_min.item(), x_max.item(), y_max.item()\n",
    "\n",
    "            cutout_img = plt.imread(img_path)[int(y_min):int(y_max), int(x_min):int(x_max)]\n",
    "            output_filename = f\"{label}-{temp_store_label_no[label]}-crop.jpg\"\n",
    "            plt.imsave(os.path.join(entity_folder, output_filename), cutout_img)\n",
    "\n",
    "def compare_images_clip(output_folder):\n",
    "    clip_model, clip_preprocess = clip.load(\"ViT-B/32\")\n",
    "    matched_paths = [os.path.join(root, file) for root, _, files in os.walk(output_folder) for file in files if file.endswith('.jpg')]\n",
    "\n",
    "    for root, _, files in os.walk(output_folder):\n",
    "        if files:\n",
    "            complete_img_path = os.path.join(root, files[0])\n",
    "            entity = ''.join(filter(lambda z: not z.isdigit(), files[0].split('-')[0]))\n",
    "            input_image = clip_preprocess(plt.imread(complete_img_path))[None]\n",
    "            input_image_features = clip_model.encode_image(input_image)\n",
    "            results = {}\n",
    "\n",
    "            for img_path in matched_paths:\n",
    "                if complete_img_path == img_path:\n",
    "                    continue\n",
    "                image_preprocess = clip_preprocess(plt.imread(img_path))[None]\n",
    "                image_features = clip_model.encode_image(image_preprocess)\n",
    "                similarity_score = (1 + (image_features @ input_image_features.T) / 2).item()\n",
    "\n",
    "                results[img_path] = similarity_score\n",
    "\n",
    "            sorted_results = dict(sorted(results.items(), key=lambda x: x[1], reverse=True))\n",
    "            top_3_results = dict(itertools.islice(sorted_results.items(), 3))\n",
    "\n",
    "            print(top_3_results)\n",
    "            temp_count = 1\n",
    "\n",
    "            for image_path in top_3_results.keys():\n",
    "                new_filename = f\"top{temp_count}-crop.jpeg\"\n",
    "                destination_path = os.path.join(root, new_filename)\n",
    "                shutil.copy(image_path, destination_path)\n",
    "                temp_count += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
